{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc, logfbank\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy.signal import resample\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pickle\n",
    "\n",
    "from plotting_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, mode='conv', nfilt=26, nfft=520, ncoef=13, rate=16_000, audio_size = .10):\n",
    "        self.mode = mode\n",
    "        self.nfilt = nfilt\n",
    "        self.nfft = nfft\n",
    "        self.ncoef = ncoef\n",
    "        self.rate = rate\n",
    "        self.audio_size = audio_size\n",
    "        self.step = int(rate*self.audio_size)\n",
    "        \n",
    "config = Config(audio_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./')\n",
    "TRAIN_FILES = Path('clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5388d14d.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c685f05f.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d20ab5.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d6665734.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7352e28f.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname      label\n",
       "0  5388d14d.wav  Saxophone\n",
       "1  c685f05f.wav  Saxophone\n",
       "2  36d20ab5.wav  Saxophone\n",
       "3  d6665734.wav  Saxophone\n",
       "4  7352e28f.wav  Saxophone"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = pd.read_csv(PATH/'instruments.csv')\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.set_index('fname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi-hat              30\n",
       "Cello               30\n",
       "Acoustic_guitar     30\n",
       "Double_bass         30\n",
       "Saxophone           30\n",
       "Bass_drum           30\n",
       "Snare_drum          30\n",
       "Violin_or_fiddle    30\n",
       "Clarinet            30\n",
       "Flute               30\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.label.value_counts() # uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lengths(df):\n",
    "    for f in df.index:\n",
    "        rate, signal = wavfile.read(TRAIN_FILES/f)\n",
    "        df.at[f, 'length'] = signal.shape[0]/rate\n",
    "\n",
    "compute_lengths(base_df)\n",
    "\n",
    "# filter out too shot audios\n",
    "base_df = base_df[base_df.length>=config.audio_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "\n",
    "idxs = np.random.rand(len(base_df))<.15\n",
    "\n",
    "df = base_df[~idxs]\n",
    "valid_df = base_df[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.label.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df),len(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training samples\n",
    "\n",
    "Unlike most other kinds of data & tasks audio data can be augmented for training. A single audio file can represent more than one sample for training. To create those samples we can samples parts of audio from the waves files. For instance one could sample 10ths of a second from a specific file.\n",
    "\n",
    "To keep the training distribution we are interested in preserving the class distribution in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    236.000000\n",
       "mean       4.700635\n",
       "std        3.446258\n",
       "min        0.518188\n",
       "25%        2.415656\n",
       "50%        3.797531\n",
       "75%        6.253625\n",
       "max       19.252437\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      4.109670\n",
       "std       3.389165\n",
       "min       0.502687\n",
       "25%       1.545000\n",
       "50%       3.159063\n",
       "75%       5.977469\n",
       "max      14.263312\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining audio files: 236\n"
     ]
    }
   ],
   "source": [
    "# classes distribution\n",
    "classes = df.label.unique().tolist()\n",
    "class_dist = df.groupby(['label'])['length'].mean() # != approach with Counter ? \n",
    "prob_dist = class_dist/class_dist.sum()\n",
    "\n",
    "# sampling parameters\n",
    "n_samples = 2 * int(df.length.sum()/config.audio_size)\n",
    "choices = np.random.choice(class_dist.index, size=n_samples, p=prob_dist)\n",
    "\n",
    "# filter out audios which len < audio_size\n",
    "df = df[df.length > config.audio_size]\n",
    "print(f'Remaining audio files: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rand_feat(n_samples, choices, df, mode='train'):\n",
    "    X = []\n",
    "    y = []\n",
    "    _min, _max = float('inf'), -float('inf')\n",
    "    for i in tqdm_notebook(range(n_samples)):\n",
    "        file = np.random.choice(df[df.label == choices[i]].index)\n",
    "        rate, wav = wavfile.read(TRAIN_FILES/file)\n",
    "#         print(wav.shape)\n",
    "        rand_start = np.random.randint(0,wav.shape[0] - config.step)\n",
    "        sample = wav[rand_start:rand_start+config.step]\n",
    "        X_sample = mfcc(sample, rate, \n",
    "                        numcep=config.ncoef, nfft=config.nfft, nfilt=config.nfilt).T\n",
    "        \n",
    "        _min = min(_min, np.amin(X_sample))\n",
    "        _max = max(_max, np.amax(X_sample))\n",
    "        \n",
    "        X.append(X_sample)\n",
    "        y.append(classes.index(choices[i]))\n",
    "    \n",
    "    if mode=='train':\n",
    "        config._min, config._max = _min, _max\n",
    "    else:\n",
    "        _min, _max = config._min, config._max\n",
    "        \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    # normalize\n",
    "    X = (X - _min)/(_max - _min)\n",
    "    \n",
    "    if config.mode == 'conv': X = np.expand_dims(X, axis=1)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0afac7501e49499b7614f3219c7d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4436), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = build_rand_feat(n_samples, choices, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc2e8535d1b4b75a27a269016850e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4436), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = build_rand_feat(n_samples, choices, valid_df, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66506747, 0.93389474, 1.4640264 , 1.34018127, 0.76351119,\n",
       "       1.64907063, 1.01743119, 3.14609929, 0.62478873, 0.84818356])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4436, 1, 13, 49), (4436,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save configuration & data\n",
    "\n",
    "def save_data(runs_dir = 'runs', nrun=None):\n",
    "    if nrun:\n",
    "        run_dir = os.path.join(runs_dir,nrun)\n",
    "    else:\n",
    "        nrun = str(len(os.listdir(runs_dir)))\n",
    "        run_dir = os.path.join(runs_dir,nrun)\n",
    "        os.mkdir(run_dir)\n",
    "    \n",
    "    objs = [X, y, config]\n",
    "    extensions = ['X.p', 'y.p', 'config.p']\n",
    "\n",
    "    to_save = zip(objs, extensions)\n",
    "\n",
    "    for obj, ext in to_save:\n",
    "        p = os.path.join(run_dir, ext)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(obj, f, protocol=2)\n",
    "    \n",
    "# save_data(nrun=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set & Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X:np.ndarray, y:np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(AudioDataset(X, y), \n",
    "                batch_size=64)\n",
    "\n",
    "valid_dl = DataLoader(AudioDataset(X_val, y_val),\n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cnn_layers = []\n",
    "\n",
    "        cnn_layers.append(nn.Conv2d(1, 16, (7,7), stride = 1, padding = 3))\n",
    "        cnn_layers.append(nn.Conv2d(16, 32, (5,5), stride = 1, padding = 2))\n",
    "        cnn_layers.append(nn.Conv2d(32, 64, (3,3), stride = 1, padding = 0))\n",
    "        cnn_layers.append(nn.Conv2d(64, 128, (3,3), stride = 1, padding = 0))\n",
    "        \n",
    "        self.cnn = nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1))\n",
    "        \n",
    "        linear_layers = nn.ModuleList()\n",
    "        \n",
    "        linear_layers.append(nn.Linear(128, 64))\n",
    "        linear_layers.append(nn.ReLU())\n",
    "        linear_layers.append(nn.Linear(64, 10))\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(*linear_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cnn(x)\n",
    "        \n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        \n",
    "        out = self.linear_layers(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_like(outs:np.ndarray, ys:np.ndarray):\n",
    "    y_oh = np.zeros(outs.shape, dtype=np.int64)\n",
    "    y_oh[range(outs.shape[0]),ys] = 1\n",
    "    return y_oh\n",
    "\n",
    "def avg_auc(outs:np.ndarray, ys:np.ndarray, weights=None):\n",
    "    outs = np.vstack(outs)\n",
    "    y_oh = one_hot_like(outs, ys)\n",
    "    aucs = []\n",
    "    for i in range(outs.shape[1]):\n",
    "        aucs.append(roc_auc_score(y_oh[:,i], outs[:,i]))\n",
    "    aucs = np.array(aucs)\n",
    "    return aucs.mean() if weights is None else (aucs*weights).sum()\n",
    "\n",
    "\n",
    "def validation(model, dl:torch.utils.data.DataLoader, weights=None):\n",
    "    model.eval()\n",
    "    outs, ys = [],[]\n",
    "    with torch.no_grad():\n",
    "        for X, y in dl:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            \n",
    "            out = model(X)\n",
    "            \n",
    "            outs.append(out.cpu().detach().numpy())\n",
    "            ys.extend(y.cpu().numpy())\n",
    "\n",
    "    return avg_auc(outs, ys, weights)\n",
    "    \n",
    "\n",
    "def train(model, epochs:int, train_dl:torch.utils.data.DataLoader, valid_dl:torch.utils.data.DataLoader=None, lr:float=3*1e-3, weights=None):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    t_weights = None if weights is None else torch.tensor(weights).float().cuda()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        model.train()\n",
    "        agg_loss, n_obs = 0, 0\n",
    "        outs, ys = [],[]\n",
    "        for X, y in train_dl:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            \n",
    "            out = model(X)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(out, y, weight=t_weights)\n",
    "            \n",
    "            agg_loss += loss.item()*y.shape[0]\n",
    "            n_obs += y.shape[0]\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            outs.append(out.cpu().detach().numpy())\n",
    "            ys.extend(y.cpu().numpy())\n",
    "            \n",
    "        tmetric = avg_auc(outs, ys)\n",
    "\n",
    "        log = f'Epoch {epoch+1}: train loss {agg_loss/n_obs:.4f} avg auc {tmetric:.4f}'\n",
    "        \n",
    "        if valid_dl:\n",
    "            vmetric = validation(model, valid_dl)\n",
    "            print(log + f' val. avg auc {vmetric:.4f}')\n",
    "        else: \n",
    "            print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02991d9dcdcd4c099276452bb8e988df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 2.2364 avg auc 0.5911 val. avg auc 0.6817\n",
      "Epoch 2: train loss 1.8540 avg auc 0.7303 val. avg auc 0.7430\n",
      "Epoch 3: train loss 1.6641 avg auc 0.7699 val. avg auc 0.7525\n",
      "Epoch 4: train loss 1.4456 avg auc 0.8095 val. avg auc 0.7665\n",
      "Epoch 5: train loss 1.2026 avg auc 0.8525 val. avg auc 0.7991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, 5, dl, valid_dl, weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca6ebd4c0ce40c99eb0eadc483031c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0738 avg auc 0.8865 val. avg auc 0.8004\n",
      "Epoch 2: train loss 0.9421 avg auc 0.8974 val. avg auc 0.8063\n",
      "Epoch 3: train loss 0.8448 avg auc 0.9044 val. avg auc 0.8069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, 3, dl, valid_dl, lr=1*1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
